{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxZDXLDCXkk_"
      },
      "source": [
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ6sKi8ZX1z4"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Finance/07.1.Training_Financial_Assertion.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLqW6FOnEvov"
      },
      "source": [
        "# Training Finance Assertion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcsTZJmZu7CQ"
      },
      "source": [
        "#Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4914b00f-abd2-4449-9a4f-737f317317be",
        "outputId": "61f458a6-bcdb-4583-afbb-4f9bbfcbcaf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting johnsnowlabs\n",
            "  Downloading johnsnowlabs-4.2.5-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.2/74.2 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spark-nlp==4.2.4\n",
            "  Downloading spark_nlp-4.2.4-py2.py3-none-any.whl (448 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.4/448.4 KB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from johnsnowlabs) (1.21.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from johnsnowlabs) (1.10.4)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting spark-nlp-display==4.1\n",
            "  Downloading spark_nlp_display-4.1-py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nlu==4.0.1rc4\n",
            "  Downloading nlu-4.0.1rc4-py3-none-any.whl (570 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m570.6/570.6 KB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyspark==3.1.2\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from johnsnowlabs) (2.25.1)\n",
            "Collecting databricks-api\n",
            "  Downloading databricks_api-0.8.0-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.8/dist-packages (from nlu==4.0.1rc4->johnsnowlabs) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.8/dist-packages (from nlu==4.0.1rc4->johnsnowlabs) (9.0.0)\n",
            "Collecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting svgwrite==1.4\n",
            "  Downloading svgwrite-1.4-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from spark-nlp-display==4.1->johnsnowlabs) (7.9.0)\n",
            "Collecting databricks-cli\n",
            "  Downloading databricks-cli-0.17.4.tar.gz (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic->johnsnowlabs) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->johnsnowlabs) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->johnsnowlabs) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->johnsnowlabs) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->johnsnowlabs) (4.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.5->nlu==4.0.1rc4->johnsnowlabs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.3.5->nlu==4.0.1rc4->johnsnowlabs) (2022.7)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli->databricks-api->johnsnowlabs) (7.1.2)\n",
            "Collecting pyjwt>=1.7.0\n",
            "  Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli->databricks-api->johnsnowlabs) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.8/dist-packages (from databricks-cli->databricks-api->johnsnowlabs) (0.8.10)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from databricks-cli->databricks-api->johnsnowlabs) (1.15.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (4.8.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (5.7.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (0.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->spark-nlp-display==4.1->johnsnowlabs) (2.0.10)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->spark-nlp-display==4.1->johnsnowlabs) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->spark-nlp-display==4.1->johnsnowlabs) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->spark-nlp-display==4.1->johnsnowlabs) (0.7.0)\n",
            "Building wheels for collected packages: pyspark, databricks-cli\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880769 sha256=61c31db9f233e1452b554f8a9a590efc65e94c54a93b02bf5b68dc0bce91ccae\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/88/9e/58ef1f74892fef590330ca0830b5b6d995ba29b44f977b3926\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for databricks-cli: filename=databricks_cli-0.17.4-py3-none-any.whl size=142894 sha256=9bcef69181bb03359ac45a883278014638ae267fcaf3fa6aed3e442795f5aac2\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/7c/6e/4bf2c1748c7ecf994ca951591de81674ed6bf633e1e337d873\n",
            "Successfully built pyspark databricks-cli\n",
            "Installing collected packages: spark-nlp, py4j, dataclasses, svgwrite, pyspark, pyjwt, jedi, colorama, databricks-cli, spark-nlp-display, nlu, databricks-api, johnsnowlabs\n",
            "Successfully installed colorama-0.4.6 databricks-api-0.8.0 databricks-cli-0.17.4 dataclasses-0.6 jedi-0.18.2 johnsnowlabs-4.2.5 nlu-4.0.1rc4 py4j-0.10.9 pyjwt-2.6.0 pyspark-3.1.2 spark-nlp-4.2.4 spark-nlp-display-4.1 svgwrite-1.4\n"
          ]
        }
      ],
      "source": [
        "# Install the johnsnowlabs library to access Spark-OCR and Spark-NLP for Healthcare, Finance, and Legal.\n",
        "! pip install johnsnowlabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JOzbTlCRNvVd",
        "outputId": "bb36e5d3-932a-46b8-e1c4-fb2a088411cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        var a = document.createElement(\"a\");\n",
              "        a.id=\"auth-btn\"\n",
              "        a.setAttribute(\"target\", \"_blank\");\n",
              "        a.href=\"https://my.johnsnowlabs.com/oauth/authorize/?client_id=sI4MKSmLHOX2Pg7XhM3McJS2oyKG5PHcp0BlANEW&response_type=code&code_challenge_method=S256&code_challenge=M_kCiY92n0EHDGZzYv3FrFczXC6SgiH3qoShFFMoIBU&redirect_uri=https%3A%2F%2Fcs2tzvdeja5-496ff2e9c6d22116-8000-colab.googleusercontent.com%2Flogin\";\n",
              "        a.style=\"padding:15px 20px;background-color:#0298d9;border-radius:7px;color:white;text-decoration:none;\"\n",
              "        a.innerText=\"Click here to Authorize on My.Johnsnowlabs.com\"\n",
              "        document.body.appendChild(a);\n",
              "        document.body.style = \"text-align:center;padding-top:15px;\"\n",
              "        a.click()\n",
              "      "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "127.0.0.1 - - [11/Jan/2023 13:25:22] \"GET /login?code=TYkvDFLO0RNSEGP7z99yh3MWq75YUF HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "document.body.removeChild(a);"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading license...\n",
            "Licenses extracted successfully\n",
            "📋 Stored John Snow Labs License in /root/.johnsnowlabs/licenses/license_number_0_for_Spark-Healthcare_Spark-OCR.json\n",
            "👷 Setting up  John Snow Labs home in /root/.johnsnowlabs, this might take a few minutes.\n",
            "Downloading 🐍+🚀 Python Library spark_nlp-4.2.4-py2.py3-none-any.whl\n",
            "Downloading 🐍+💊 Python Library spark_nlp_jsl-4.2.4-py3-none-any.whl\n",
            "Downloading 🫘+🚀 Java Library spark-nlp-assembly-4.2.4.jar\n",
            "Downloading 🫘+💊 Java Library spark-nlp-jsl-4.2.4.jar\n",
            "🙆 JSL Home setup in /root/.johnsnowlabs\n",
            "Installing /root/.johnsnowlabs/py_installs/spark_nlp_jsl-4.2.4-py3-none-any.whl to /usr/bin/python3\n",
            "Running: /usr/bin/python3 -m pip install /root/.johnsnowlabs/py_installs/spark_nlp_jsl-4.2.4-py3-none-any.whl\n",
            "Installed 1 products:\n",
            "💊 Spark-Healthcare==4.2.4 installed! ✅ Heal the planet with NLP! \n"
          ]
        }
      ],
      "source": [
        "from johnsnowlabs import nlp, finance\n",
        "# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n",
        "# Make sure to restart your notebook afterwards for changes to take effect\n",
        "nlp.install(force_browser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ki72qJQGPOz"
      },
      "source": [
        "## Start Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TZIjuI3zN1Oi",
        "outputId": "bf4bff50-dff1-4a37-f7ee-d435d3ec8780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "👌 Launched \u001b[92mcpu optimized\u001b[39m session with with: 🚀Spark-NLP==4.2.4, 💊Spark-Healthcare==4.2.4, running on ⚡ PySpark==3.1.2\n"
          ]
        }
      ],
      "source": [
        "# Automatically load license data and start a session with all jars user has access to\n",
        "spark = nlp.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "GlmdmVsK0npI",
        "outputId": "a5b6a2c8-acc2-4bf2-f455-0cf86df5c269"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff940579fd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d4ae988aec6f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>John-Snow-Labs-Spark-Session 🚀 with Jars for: 🚀Spark-NLP==4.2.4, 💊Spark-Healthcare==4.2.4, running on ⚡ PySpark==3.1.2</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYBQyxEd0uR0"
      },
      "source": [
        "#Data Prep "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AVBmGFcQ03La"
      },
      "outputs": [],
      "source": [
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings_JSL/Finance/data/assertion_df.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8iJF_HCw1Lgh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "training_df = pd.read_csv('./assertion_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JREBeTzb8ov-",
        "outputId": "10c448fc-8109-4a82-80cc-2a4af3f63a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------+-----+---+\n",
            "|                text|              target|   label|start|end|\n",
            "+--------------------+--------------------+--------+-----+---+\n",
            "|CEC ENTERTAINMENT...|CEC ENTERTAINMENT...|negative|    0|  2|\n",
            "|CEC ENTERTAINMENT...|       GEO GROUP INC|positive|    6|  8|\n",
            "|BRAVE ASSET MANAG...|Mondelez Internat...|positive|    6| 10|\n",
            "|BRAVE ASSET MANAG...|BRAVE ASSET MANAG...|positive|    0|  3|\n",
            "|Compound Natural ...|Compound Natural ...|negative|    0|  4|\n",
            "|Compound Natural ...|AMERICAN ELECTRIC...|positive|    9| 13|\n",
            "|Marijuana Co of A...|PVM International...|positive|   10| 14|\n",
            "|Marijuana Co of A...|Marijuana Co of A...|positive|    0|  6|\n",
            "|NORTEK INC is not...|          NORTEK INC|negative|    0|  1|\n",
            "|NORTEK INC is not...|EN2GO INTERNATION...|positive|    6|  8|\n",
            "|QUALCOMM INC/DE i...| CANNAPOWDER , INC .|positive|    8| 11|\n",
            "|QUALCOMM INC/DE i...|     QUALCOMM INC/DE|positive|    0|  1|\n",
            "|TransDigm Group I...| TransDigm Group INC|negative|    0|  2|\n",
            "|TransDigm Group I...|         ABIOMED INC|positive|    9| 10|\n",
            "|Fundrise Income e...|MIDDLETON & CO IN...|positive|    9| 12|\n",
            "|Fundrise Income e...|Fundrise Income e...|negative|    0|  5|\n",
            "|Nexeo Solutions ,...|Nexeo Solutions ,...|negative|    0|  4|\n",
            "|Nexeo Solutions ,...|ARCA biopharma , ...|positive|    8| 12|\n",
            "|Angie's List , In...|        RC-1 , Inc .|positive|   11| 14|\n",
            "|Angie's List , In...|Angie's List , Inc .|negative|    0|  4|\n",
            "+--------------------+--------------------+--------+-----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "training_data = spark.createDataFrame(training_df)\n",
        "training_data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET8GD3y3-17e",
        "outputId": "0750fe37-9900-4cca-d464-7e25a36e512b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- target: string (nullable = true)\n",
            " |-- label: string (nullable = true)\n",
            " |-- start: long (nullable = true)\n",
            " |-- end: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "training_data.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6xa4jp8Szs0",
        "outputId": "04a04237-23a3-4626-abd6-51bef40ff907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8.25 ms, sys: 6.46 ms, total: 14.7 ms\n",
            "Wall time: 750 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "%time training_data.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxcHD_Q_-_lD",
        "outputId": "a6270364-d0d2-41bf-c9cd-7aff03f7e6ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 71\n",
            "Test Dataset Count: 27\n"
          ]
        }
      ],
      "source": [
        "(train_data, test_data) = training_data.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(train_data.count()))\n",
        "print(\"Test Dataset Count: \" + str(test_data.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFN_BuHU84HF",
        "outputId": "181f327b-8a66-4a13-fc16-dd3acce7c098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------+-----+---+\n",
            "|                text|              target|   label|start|end|\n",
            "+--------------------+--------------------+--------+-----+---+\n",
            "|CEC ENTERTAINMENT...|CEC ENTERTAINMENT...|negative|    0|  2|\n",
            "|CEC ENTERTAINMENT...|       GEO GROUP INC|positive|    6|  8|\n",
            "|BRAVE ASSET MANAG...|BRAVE ASSET MANAG...|positive|    0|  3|\n",
            "|BRAVE ASSET MANAG...|Mondelez Internat...|positive|    6| 10|\n",
            "|Compound Natural ...|AMERICAN ELECTRIC...|positive|    9| 13|\n",
            "|NORTEK INC is not...|EN2GO INTERNATION...|positive|    6|  8|\n",
            "|NORTEK INC is not...|          NORTEK INC|negative|    0|  1|\n",
            "|QUALCOMM INC/DE i...| CANNAPOWDER , INC .|positive|    8| 11|\n",
            "|QUALCOMM INC/DE i...|     QUALCOMM INC/DE|positive|    0|  1|\n",
            "|TransDigm Group I...|         ABIOMED INC|positive|    9| 10|\n",
            "|TransDigm Group I...| TransDigm Group INC|negative|    0|  2|\n",
            "|Nexeo Solutions ,...|ARCA biopharma , ...|positive|    8| 12|\n",
            "|Nexeo Solutions ,...|Nexeo Solutions ,...|negative|    0|  4|\n",
            "|SHOE CARNIVAL INC...|     Alliqua , Inc .|positive|    6|  9|\n",
            "|SHOE CARNIVAL INC...|   SHOE CARNIVAL INC|negative|    0|  2|\n",
            "|WACCAMAW BANKSHAR...|HOWLAND CAPITAL M...|positive|    5|  8|\n",
            "|WACCAMAW BANKSHAR...|WACCAMAW BANKSHAR...|positive|    0|  2|\n",
            "|ALEXANDRIA REAL E...|ALEXANDRIA REAL E...|negative|    0|  4|\n",
            "|ATMI INC is eligi...|            ATMI INC|positive|    0|  1|\n",
            "|Artificial Intell...|Artificial Intell...|negative|    0|  5|\n",
            "+--------------------+--------------------+--------+-----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_data.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.show()"
      ],
      "metadata": {
        "id": "qBt7gVbm7DBc",
        "outputId": "ca63586b-7313-4324-a577-12b090925b57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------+-----+---+\n",
            "|                text|              target|   label|start|end|\n",
            "+--------------------+--------------------+--------+-----+---+\n",
            "|Compound Natural ...|Compound Natural ...|negative|    0|  4|\n",
            "|Marijuana Co of A...|Marijuana Co of A...|positive|    0|  6|\n",
            "|Marijuana Co of A...|PVM International...|positive|   10| 14|\n",
            "|Fundrise Income e...|Fundrise Income e...|negative|    0|  5|\n",
            "|Fundrise Income e...|MIDDLETON & CO IN...|positive|    9| 12|\n",
            "|Angie's List , In...|Angie's List , Inc .|negative|    0|  4|\n",
            "|Angie's List , In...|        RC-1 , Inc .|positive|   11| 14|\n",
            "|ALEXANDRIA REAL E...|            CDEX INC|positive|    9| 10|\n",
            "|ATMI INC is eligi...|NEAH POWER SYSTEM...|positive|    5| 10|\n",
            "|Artificial Intell...| APA OPTICS INC /MN/|positive|   10| 13|\n",
            "|Mountain Capital ...|            COSI INC|positive|   11| 12|\n",
            "|Mountain Capital ...|Mountain Capital ...|negative|    0|  5|\n",
            "|QUAINT OAK BANCOR...|QUAINT OAK BANCOR...|positive|    0|  3|\n",
            "|MGP INGREDIENTS I...|    LAND O LAKES INC|positive|    6|  9|\n",
            "|3AM TECHNOLOGIES ...|3AM TECHNOLOGIES INC|negative|    0|  2|\n",
            "|SURMODICS INC is ...|       SURMODICS INC|positive|    0|  1|\n",
            "|TGC INDUSTRIES IN...|OMNIVISION TECHNO...|positive|    5|  7|\n",
            "|ACURA PHARMACEUTI...|Fig Publishing , ...|positive|    7| 11|\n",
            "|Fundrise Income e...|RBC LIFE SCIENCES...|positive|    9| 14|\n",
            "|AMERICAN CENTURY ...| Charmed Homes Inc .|positive|    9| 12|\n",
            "+--------------------+--------------------+--------+-----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WZDqlZA_kmb"
      },
      "source": [
        "#Using Bert Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qfJh8ap_nI2",
        "outputId": "8872e005-7c4e-41c8-ef2b-06169b70707d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_embeddings_sec_bert_base download started this may take some time.\n",
            "Approximate size to download 390.4 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "bert_embeddings = nlp.BertEmbeddings.pretrained(\"bert_embeddings_sec_bert_base\", \"en\") \\\n",
        "  .setInputCols(\"document\", \"token\") \\\n",
        "  .setOutputCol(\"embeddings\")\\\n",
        "  .setMaxSentenceLength(512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fe0957BT_rcy"
      },
      "outputs": [],
      "source": [
        "document = nlp.DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "chunk = nlp.Doc2Chunk()\\\n",
        "    .setInputCols(\"document\")\\\n",
        "    .setOutputCol(\"doc_chunk\")\\\n",
        "    .setChunkCol(\"target\")\\\n",
        "    .setStartCol(\"start\")\\\n",
        "    .setStartColByTokenIndex(True)\\\n",
        "    .setFailOnMissing(False)\\\n",
        "    .setLowerCase(False)\n",
        "\n",
        "token = nlp.Tokenizer()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('token')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFTO0PlI9-3e"
      },
      "source": [
        "We save the test data in parquet format to use in `AssertionDLApproach()`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M9u4c65G9VaC"
      },
      "outputs": [],
      "source": [
        "assertion_pipeline = nlp.Pipeline(\n",
        "    stages = [\n",
        "    document,\n",
        "    chunk,\n",
        "    token,\n",
        "    bert_embeddings])\n",
        "\n",
        "assertion_test_data = assertion_pipeline.fit(test_data).transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-UREmtI9Vd3",
        "outputId": "e27ba7af-4941-4cd8-ffaf-b03c2f996545"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text',\n",
              " 'target',\n",
              " 'label',\n",
              " 'start',\n",
              " 'end',\n",
              " 'document',\n",
              " 'doc_chunk',\n",
              " 'token',\n",
              " 'embeddings']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "assertion_test_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kBaiXx78BTLT"
      },
      "outputs": [],
      "source": [
        "assertion_test_data.write.mode('overwrite').parquet('test_data.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tFhN6evk9ViI"
      },
      "outputs": [],
      "source": [
        "assertion_train_data = assertion_pipeline.fit(training_data).transform(training_data)\n",
        "assertion_train_data.write.mode('overwrite').parquet('train_data.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtxnrvcA9VlN",
        "outputId": "f04036e7-66db-432f-88a6-160b2b8d9cd2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text',\n",
              " 'target',\n",
              " 'label',\n",
              " 'start',\n",
              " 'end',\n",
              " 'document',\n",
              " 'doc_chunk',\n",
              " 'token',\n",
              " 'embeddings']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "assertion_train_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTishXbut1MS"
      },
      "source": [
        "##Graph setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1hTawAHemzGn",
        "outputId": "51f23f0f-99fa-470f-ac64-2d59450979de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.6/489.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 KB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow==2.7.0\n",
        "!pip install -q tensorflow-addons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ShZT8BBo4FY"
      },
      "source": [
        "We will use TFGraphBuilder annotator which can be used to create graphs in the model training pipeline. \n",
        "\n",
        "TFGraphBuilder inspects the data and creates the proper graph if a suitable version of TensorFlow (<= 2.7 ) is available. The graph is stored in the defined folder and loaded by the approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "B8xGC2FLISvU"
      },
      "outputs": [],
      "source": [
        "from johnsnowlabs import nlp, finance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XhU0L1OXdaLN"
      },
      "outputs": [],
      "source": [
        "graph_folder= \"./tf_graphs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "miNgoTjio0mL"
      },
      "outputs": [],
      "source": [
        "assertion_graph_builder =  finance.TFGraphBuilder()\\\n",
        "    .setModelName(\"assertion_dl\")\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setLabelColumn(\"label\")\\\n",
        "    .setGraphFolder(graph_folder)\\\n",
        "    .setGraphFile(\"assertion_graph.pb\")\\\n",
        "    .setMaxSequenceLength(1200)\\\n",
        "    .setHiddenUnitsNumber(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D0Ng7nMUjJa"
      },
      "source": [
        "**Setting the Scope Window (Target Area) Dynamically in Assertion Status Detection Models**\n",
        "\n",
        "\n",
        "This parameter allows you to train the Assertion Status Models to focus on specific context windows when resolving the status of a NER chunk. The window is in format `[X,Y]` being `X` the number of tokens to consider on the left of the chunk, and `Y` the max number of tokens to consider on the right. Let’s take a look at what different windows mean:\n",
        "\n",
        "\n",
        "*   By default, the window is `[-1,-1]` which means that the Assertion Status will look at all of the tokens in the sentence/document (up to a maximum of tokens set in `setMaxSentLen()` ).\n",
        "*   `[0,0]` means “don’t pay attention to any token except the ner_chunk”, what basically is not considering any context for the Assertion resolution.\n",
        "*   `[9,15]` is what empirically seems to be the best baseline, meaning that we look up to 9 tokens on the left and 15 on the right of the ner chunk to understand the context and resolve the status.\n",
        "\n",
        "\n",
        "Check this [Scope Window Tuning Assertion Status Detection notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.1.Scope_window_tuning_assertion_status_detection.ipynb)  that illustrates the effect of the different windows and how to properly fine-tune your AssertionDLModels to get the best of them.\n",
        "\n",
        "In our case, the best Scope Window is around [10,10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BQxGbYks91go"
      },
      "outputs": [],
      "source": [
        "scope_window = [50, 50]\n",
        "\n",
        "assertionStatus = finance.AssertionDLApproach()\\\n",
        "    .setLabelCol(\"label\")\\\n",
        "    .setInputCols(\"document\", \"doc_chunk\", \"embeddings\")\\\n",
        "    .setOutputCol(\"assertion\")\\\n",
        "    .setBatchSize(128)\\\n",
        "    .setLearningRate(0.001)\\\n",
        "    .setEpochs(2)\\\n",
        "    .setStartCol(\"start\")\\\n",
        "    .setEndCol(\"end\")\\\n",
        "    .setMaxSentLen(1200)\\\n",
        "    .setEnableOutputLogs(True)\\\n",
        "    .setOutputLogsPath('training_logs/')\\\n",
        "    .setGraphFolder(graph_folder)\\\n",
        "    .setGraphFile(f\"{graph_folder}/assertion_graph.pb\")\\\n",
        "    .setTestDataset(path=\"test_data.parquet\", read_as='SPARK', options={'format': 'parquet'})\\\n",
        "    .setScopeWindow(scope_window)\n",
        "    #.setValidationSplit(0.2)\\    \n",
        "    #.setDropout(0.1)\\    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "T2MZLeCYATrS"
      },
      "outputs": [],
      "source": [
        "clinical_assertion_pipeline = nlp.Pipeline(\n",
        "    stages = [\n",
        "    #document,\n",
        "    #chunk,\n",
        "    #token,\n",
        "    #embeddings,\n",
        "    assertion_graph_builder,\n",
        "    assertionStatus])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIvnuaQP91j8",
        "outputId": "62124996-0c1b-4404-ed3e-ad0f302cb483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- target: string (nullable = true)\n",
            " |-- label: string (nullable = true)\n",
            " |-- start: long (nullable = true)\n",
            " |-- end: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "training_data.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ueJz0aiJ_7l4"
      },
      "outputs": [],
      "source": [
        "assertion_train_data = spark.read.parquet('train_data.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assertion_train_data.groupBy('label').count().show()"
      ],
      "metadata": {
        "id": "sXpFjULc8sjA",
        "outputId": "5bd5358e-35b2-4a96-cc26-f8468a832216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|   label|count|\n",
            "+--------+-----+\n",
            "|positive|   71|\n",
            "|negative|   27|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "j1NCZ89T_7ol",
        "outputId": "256e3500-2370-4b8e-a2ba-8763496b1f96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF Graph Builder configuration:\n",
            "Model name: assertion_dl\n",
            "Graph folder: ./tf_graphs\n",
            "Graph file name: assertion_graph.pb\n",
            "Build params: {'n_classes': 2, 'feat_size': 768, 'max_seq_len': 1200, 'n_hidden': 25}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device mapping: no known devices.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn.py:229: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/rnn.py:441: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:766: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device mapping: no known devices.\n",
            "assertion_dl graph exported to ./tf_graphs/assertion_graph.pb\n",
            "CPU times: user 9.23 s, sys: 4.52 s, total: 13.8 s\n",
            "Wall time: 32.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "assertion_model = clinical_assertion_pipeline.fit(assertion_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30SmcTiSpnWa"
      },
      "source": [
        "Checking the results saved in the log file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kOiu1vuspKut",
        "outputId": "f356d1e9-1173-4b16-84ec-58b3dea0860f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AssertionDLApproach_00411fdbdc10.log']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "log_files = os.listdir(\"./training_logs\")\n",
        "log_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CcQV0-fIrJHz",
        "outputId": "109e7590-aeda-4c0f-ee0c-848e5e850e17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name of the selected graph: ./tf_graphs/assertion_graph.pb\n",
            "Training started, trainExamples: 98\n",
            "\n",
            "\n",
            "Epoch: 0 started, learning rate: 0.001, dataset size: 98\n",
            "Done, 9.570665722 total training loss: 0.80261874, avg training loss: 0.80261874, batches: 1\n",
            "Quality on test dataset: \n",
            "time to finish evaluation: 2.13s\n",
            "Total test loss: 0.7050\tAvg test loss: 0.7050\n",
            "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
            "positive\t 0\t 0\t 18\t 0.0\t 0.0\t 0.0\n",
            "negative\t 9\t 18\t 0\t 0.33333334\t 1.0\t 0.5\n",
            "tp: 9 fp: 18 fn: 18 labels: 2\n",
            "Macro-average\t prec: 0.16666667, rec: 0.5, f1: 0.25\n",
            "Micro-average\t prec: 0.33333334, rec: 0.33333334, f1: 0.33333334\n",
            "\n",
            "\n",
            "Epoch: 1 started, learning rate: 9.5E-4, dataset size: 98\n",
            "Done, 7.344641001 total training loss: 0.70753396, avg training loss: 0.70753396, batches: 1\n",
            "Quality on test dataset: \n",
            "time to finish evaluation: 1.77s\n",
            "Total test loss: 0.6590\tAvg test loss: 0.6590\n",
            "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
            "positive\t 18\t 9\t 0\t 0.6666667\t 1.0\t 0.8\n",
            "negative\t 0\t 0\t 9\t 0.0\t 0.0\t 0.0\n",
            "tp: 18 fp: 9 fn: 9 labels: 2\n",
            "Macro-average\t prec: 0.33333334, rec: 0.5, f1: 0.4\n",
            "Micro-average\t prec: 0.6666667, rec: 0.6666667, f1: 0.6666667\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open(\"./training_logs/\"+log_files[0]) as log_file:\n",
        "    print(log_file.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "bgcG00nT91nn"
      },
      "outputs": [],
      "source": [
        "assertion_test_data = spark.read.parquet('test_data.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-k2WrFkRyQyP",
        "outputId": "110d6466-fd3a-4459-fa53-44c8dea2b3bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+\n",
            "|   label|    result|\n",
            "+--------+----------+\n",
            "|negative|[positive]|\n",
            "|positive|[positive]|\n",
            "|positive|[positive]|\n",
            "|negative|[positive]|\n",
            "|positive|[positive]|\n",
            "|negative|[positive]|\n",
            "|positive|[positive]|\n",
            "|positive|[positive]|\n",
            "|positive|[positive]|\n",
            "|positive|[positive]|\n",
            "|negative|[positive]|\n",
            "|negative|[positive]|\n",
            "|positive|[positive]|\n",
            "|positive|[positive]|\n",
            "|negative|[positive]|\n",
            "|positive|[positive]|\n",
            "|negative|[positive]|\n",
            "|positive|[positive]|\n",
            "|positive|[positive]|\n",
            "|negative|[positive]|\n",
            "+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "preds = assertion_model.transform(assertion_test_data).select('label','assertion.result')\n",
        "\n",
        "preds.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "4yI73lwG2xk5"
      },
      "outputs": [],
      "source": [
        "preds_df = preds.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "yRXZFGlQ3Z2U",
        "outputId": "fef2a340-41d3-44d1-a24e-6814b0fbb2e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 896
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       label    result\n",
              "0   negative  positive\n",
              "1   positive  positive\n",
              "2   positive  positive\n",
              "3   negative  positive\n",
              "4   positive  positive\n",
              "5   negative  positive\n",
              "6   positive  positive\n",
              "7   positive  positive\n",
              "8   positive  positive\n",
              "9   positive  positive\n",
              "10  negative  positive\n",
              "11  negative  positive\n",
              "12  positive  positive\n",
              "13  positive  positive\n",
              "14  negative  positive\n",
              "15  positive  positive\n",
              "16  negative  positive\n",
              "17  positive  positive\n",
              "18  positive  positive\n",
              "19  negative  positive\n",
              "20  positive  positive\n",
              "21  positive  positive\n",
              "22  positive  positive\n",
              "23  negative  positive\n",
              "24  positive  positive\n",
              "25  positive  positive\n",
              "26  positive  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60b963e7-20d6-426c-a728-5d0a820bc4c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>negative</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>positive</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60b963e7-20d6-426c-a728-5d0a820bc4c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-60b963e7-20d6-426c-a728-5d0a820bc4c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-60b963e7-20d6-426c-a728-5d0a820bc4c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "preds_df[\"result\"] = preds_df[\"result\"].apply(lambda x: x[0] if len(x) else pd.NA)\n",
        "preds_df.dropna(inplace=True)\n",
        "\n",
        "preds_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "7IVTZKwUvLTX",
        "outputId": "e9fb549e-e0b8-4456-f9e9-c8825f043767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1hb1kyGAE0Gn",
        "outputId": "00880cc1-19af-4d71-8b17-d6e346ca5786",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00         9\n",
            "    positive       0.67      1.00      0.80        18\n",
            "\n",
            "    accuracy                           0.67        27\n",
            "   macro avg       0.33      0.50      0.40        27\n",
            "weighted avg       0.44      0.67      0.53        27\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We are going to use sklearn to evalute the results on test dataset\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print (classification_report( preds_df['label'], preds_df['result']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuJ5YZ9sXU13"
      },
      "source": [
        "###Saving the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "KBcoOwvwXV8p",
        "outputId": "ef16904b-719c-4604-c0b7-9226f3dae4d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TFGraphBuilderModel_2ba34934c1c2, FINANCE-ASSERTION_DL_a84ead075c86]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "assertion_model.stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ioMW1jSrA-wg",
        "outputId": "0b8d4ce3-e3d0-4a16-f929-ee0d10482b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: fields/ (stored 0%)\n",
            "  adding: fields/datasetParams/ (stored 0%)\n",
            "  adding: fields/datasetParams/part-00024 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00007 (deflated 26%)\n",
            "  adding: fields/datasetParams/.part-00016.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00039.crc (deflated 44%)\n",
            "  adding: fields/datasetParams/.part-00026.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00020.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00032.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00002.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00015 (deflated 26%)\n",
            "  adding: fields/datasetParams/part-00017 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00036 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00000.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00036.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00013.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00016 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00034 (deflated 26%)\n",
            "  adding: fields/datasetParams/.part-00019.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00021.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00024.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00019 (deflated 26%)\n",
            "  adding: fields/datasetParams/.part-00001.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00020 (deflated 26%)\n",
            "  adding: fields/datasetParams/.part-00005.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00006 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00014.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00035 (deflated 26%)\n",
            "  adding: fields/datasetParams/.part-00009.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00027.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00037 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00031.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00017.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00011 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00008 (deflated 26%)\n",
            "  adding: fields/datasetParams/part-00018 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00006.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00025.crc (stored 0%)\n",
            "  adding: fields/datasetParams/._SUCCESS.crc (stored 0%)\n",
            "  adding: fields/datasetParams/_SUCCESS (stored 0%)\n",
            "  adding: fields/datasetParams/part-00039 (deflated 95%)\n",
            "  adding: fields/datasetParams/part-00013 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00032 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00028.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00004 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00008.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00022 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00011.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00030 (deflated 26%)\n",
            "  adding: fields/datasetParams/.part-00022.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00029 (deflated 26%)\n",
            "  adding: fields/datasetParams/part-00003 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00005 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00030.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00031 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00027 (deflated 26%)\n",
            "  adding: fields/datasetParams/.part-00018.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00001 (deflated 26%)\n",
            "  adding: fields/datasetParams/.part-00004.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00033.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00025 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00012.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00038.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00029.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00002 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00035.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00015.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00037.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00038 (deflated 26%)\n",
            "  adding: fields/datasetParams/part-00033 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00000 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00021 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00010 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00023 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00034.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00023.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00007.crc (stored 0%)\n",
            "  adding: fields/datasetParams/.part-00003.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00014 (deflated 27%)\n",
            "  adding: fields/datasetParams/part-00009 (deflated 27%)\n",
            "  adding: fields/datasetParams/.part-00010.crc (stored 0%)\n",
            "  adding: fields/datasetParams/part-00012 (deflated 26%)\n",
            "  adding: fields/datasetParams/part-00028 (deflated 26%)\n",
            "  adding: fields/datasetParams/part-00026 (deflated 27%)\n",
            "  adding: metadata/ (stored 0%)\n",
            "  adding: metadata/.part-00000.crc (stored 0%)\n",
            "  adding: metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: metadata/_SUCCESS (stored 0%)\n",
            "  adding: metadata/part-00000 (deflated 39%)\n",
            "  adding: tensorflow (deflated 39%)\n"
          ]
        }
      ],
      "source": [
        "# Save a Spark NLP model\n",
        "assertion_model.stages[-1].write().overwrite().save('Assertion')\n",
        "\n",
        "# cd into saved dir and zip\n",
        "! cd /content/Assertion ; zip -r /content/Assertion.zip *"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
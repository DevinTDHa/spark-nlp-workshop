{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"I08sFJYCxR0Z"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","metadata":{"id":"LKI5K1wQrSe9"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Healthcare/25.Date_Normalizer.ipynb)"]},{"cell_type":"markdown","source":["# 25. Date Normalizer"],"metadata":{"id":"YiRjfE_SHmQI"}},{"cell_type":"markdown","metadata":{"id":"okhT7AcXxben"},"source":["## Colab Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELqzaf32MT6E"},"outputs":[],"source":["# Install the johnsnowlabs library to access Spark-OCR and Spark-NLP for Healthcare, Finance, and Legal.\n","! pip install -q johnsnowlabs "]},{"cell_type":"code","source":["from google.colab import files\n","print('Please Upload your John Snow Labs License using the button below')\n","license_keys = files.upload()"],"metadata":{"id":"RO2dIA414yL_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from johnsnowlabs import * \n","# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n","# Make sure to restart your notebook afterwards for changes to take effect\n","jsl.install()"],"metadata":{"id":"dmcB5zVBHZO8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from johnsnowlabs import * \n","# Automatically load license data and start a session with all jars user has access to\n","spark = jsl.start()"],"metadata":{"id":"lQ8-BI-_5QjG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kqlcNe7FJr31"},"source":["## **Date Normalizer**\n","\n","New Annotator that transforms chunks Dates to a normalized Date with format YYYY/MM/DD. This annotator identifies dates in chunk annotations and transforms those dates to the format YYYY/MM/DD. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"YPvA603jL3TV"},"source":["We are going to create a chunks dates with different formats:"]},{"cell_type":"code","metadata":{"id":"Cnf_s2yFLpXC"},"source":["dates = [\n","'08/02/2018',\n","'11/2018',\n","'11/01/2018',\n","'12Mar2021',\n","'Jan 30, 2018',\n","'13.04.1999', \n","'3April 2020',\n","]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SW-ND2BjONF_"},"source":["df_dates = spark.createDataFrame(dates,StringType()).toDF('ner_chunk')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JiIqDdHDPkQV"},"source":["We are going to transform that text to documents in spark-nlp."]},{"cell_type":"code","metadata":{"id":"13wTh48FPcG-"},"source":["document_assembler = nlp.DocumentAssembler().setInputCol('ner_chunk').setOutputCol('document')\n","documents_DF = document_assembler.transform(df_dates)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VelhLulcPxbx"},"source":["After that we are going to transform that documents to chunks."]},{"cell_type":"code","metadata":{"id":"lR6bSlXTPvqK"},"source":["chunks_df = nlp.map_annotations_col(documents_DF.select(\"document\",\"ner_chunk\"),\n","                    lambda x: [Annotation('chunk', a.begin, a.end, a.result, a.metadata, a.embeddings) for a in x], \"document\",\n","                    \"chunk_date\", \"chunk\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3NqyhuNCQWC2","outputId":"3a07c0d7-d0b3-4979-83fc-b629bcc8f6fb","executionInfo":{"status":"ok","timestamp":1665140792260,"user_tz":-330,"elapsed":4927,"user":{"displayName":"Gadde Sai Shailesh","userId":"15899666125549248205"}}},"source":["chunks_df.select('chunk_date').show(truncate=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------------------------------------+\n","|chunk_date                                         |\n","+---------------------------------------------------+\n","|[{chunk, 0, 9, 08/02/2018, {sentence -> 0}, []}]   |\n","|[{chunk, 0, 6, 11/2018, {sentence -> 0}, []}]      |\n","|[{chunk, 0, 9, 11/01/2018, {sentence -> 0}, []}]   |\n","|[{chunk, 0, 8, 12Mar2021, {sentence -> 0}, []}]    |\n","|[{chunk, 0, 11, Jan 30, 2018, {sentence -> 0}, []}]|\n","|[{chunk, 0, 9, 13.04.1999, {sentence -> 0}, []}]   |\n","|[{chunk, 0, 10, 3April 2020, {sentence -> 0}, []}] |\n","+---------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"iE1wA-Y1QyeI"},"source":["Now we are going to normalize those chunks using the DateNormalizer."]},{"cell_type":"code","metadata":{"id":"k9E0T8mjQo0Y"},"source":["date_normalizer = medical.DateNormalizer().setInputCols('chunk_date').setOutputCol('date')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JVX_QlglR9yR"},"source":["date_normalized_df = date_normalizer.transform(chunks_df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8IxAZxXRihE"},"source":["We are going to show how the date is normalized."]},{"cell_type":"code","metadata":{"id":"LB2yoHDqRVy-","executionInfo":{"status":"ok","timestamp":1665140793730,"user_tz":-330,"elapsed":1472,"user":{"displayName":"Gadde Sai Shailesh","userId":"15899666125549248205"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9512a4d0-4f30-4fbf-9d45-e7ed8b59956b"},"source":["dateNormalizedClean = date_normalized_df.selectExpr(\"ner_chunk\",\"date.result as dateresult\",\"date.metadata as metadata\")\n","\n","dateNormalizedClean.withColumn(\"dateresult\", dateNormalizedClean[\"dateresult\"]\n","                               .getItem(0)).withColumn(\"metadata\", dateNormalizedClean[\"metadata\"]\n","                                                       .getItem(0)['normalized']).show(truncate=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------+----------+--------+\n","|ner_chunk   |dateresult|metadata|\n","+------------+----------+--------+\n","|08/02/2018  |2018/08/02|true    |\n","|11/2018     |2018/11/DD|true    |\n","|11/01/2018  |2018/11/01|true    |\n","|12Mar2021   |2021/03/12|true    |\n","|Jan 30, 2018|2018/01/30|true    |\n","|13.04.1999  |1999/04/13|true    |\n","|3April 2020 |2020/04/03|true    |\n","+------------+----------+--------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"dwgt4QtgSf6g"},"source":["## Relative Date\n","\n","We can configure the `anchorDateYear`,`anchorDateMonth` and `anchorDateDay` for the relatives dates."]},{"cell_type":"code","metadata":{"id":"gxHErp-K7ssl"},"source":["rel_dates = [\n","'next monday',\n","'today',\n","'next week'\n","]\n","\n","rel_dates_df = spark.createDataFrame(rel_dates,StringType()).toDF('ner_chunk')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JN4QVnrD7ssu"},"source":["rel_documents_DF = document_assembler.transform(rel_dates_df)\n","\n","rel_chunks_df = nlp.map_annotations_col(rel_documents_DF.select(\"document\",\"ner_chunk\"),\n","                    lambda x: [Annotation('chunk', a.begin, a.end, a.result, a.metadata, a.embeddings) for a in x], \"document\",\n","                    \"chunk_date\", \"chunk\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QYnWQMSj7ssw","executionInfo":{"status":"ok","timestamp":1665140794798,"user_tz":-330,"elapsed":1070,"user":{"displayName":"Gadde Sai Shailesh","userId":"15899666125549248205"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"468ab6af-1169-473a-f99c-493c654628af"},"source":["rel_chunks_df.select('chunk_date').show(truncate=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------------------------------------+\n","|chunk_date                                        |\n","+--------------------------------------------------+\n","|[{chunk, 0, 10, next monday, {sentence -> 0}, []}]|\n","|[{chunk, 0, 4, today, {sentence -> 0}, []}]       |\n","|[{chunk, 0, 8, next week, {sentence -> 0}, []}]   |\n","+--------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"-WrTv3SLTIe7"},"source":["In the following example we will use as a relative date 2021/02/16, to make that possible we need to set up the `anchorDateYear` to 2021, the `anchorDateMonth` to 2 and the `anchorDateDay` to 16. We will show you the configuration with the following example."]},{"cell_type":"code","metadata":{"id":"zttFZDgJSdZi"},"source":["rel_date_normalizer = medical.DateNormalizer()\\\n","                        .setInputCols('chunk_date')\\\n","                        .setOutputCol('date')\\\n","                        .setAnchorDateDay(16)\\\n","                        .setAnchorDateMonth(2)\\\n","                        .setAnchorDateYear(2021)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HvBFcvVnUge3","executionInfo":{"status":"ok","timestamp":1665140795424,"user_tz":-330,"elapsed":627,"user":{"displayName":"Gadde Sai Shailesh","userId":"15899666125549248205"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c61406de-62ad-4962-c3da-c094a8a803b2"},"source":["rel_date_normalized_df = rel_date_normalizer.transform(rel_chunks_df)\n","relDateNormalizedClean = rel_date_normalized_df.selectExpr(\"ner_chunk\",\"date.result as dateresult\",\"date.metadata as metadata\")\n","relDateNormalizedClean.withColumn(\"dateresult\", relDateNormalizedClean[\"dateresult\"].getItem(0))\\\n","                      .withColumn(\"metadata\", relDateNormalizedClean[\"metadata\"].getItem(0)['normalized']).show(truncate=False)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+----------+--------+\n","|ner_chunk  |dateresult|metadata|\n","+-----------+----------+--------+\n","|next monday|2021/02/22|true    |\n","|today      |2021/02/16|true    |\n","|next week  |2021/02/23|true    |\n","+-----------+----------+--------+\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"HiQke1C2VGuH"},"source":["As you see the relatives dates like `next monday` , `today` and `next week` takes the `2021/02/16` as reference date.\n"]}]}